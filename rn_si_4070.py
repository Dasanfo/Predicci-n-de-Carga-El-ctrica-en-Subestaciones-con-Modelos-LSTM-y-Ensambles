# -*- coding: utf-8 -*-
"""RN SI 4070.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12Ae9eAMZ3zOlf1-5UfAT0mHupWffJ4rS
"""

# ===========================================
#  IMPORTACIONES
# ===========================================
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from math import sqrt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Dense
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# ------------------------------
# 1. Cargar y preparar datos
# ------------------------------
df = pd.read_csv("/content/powerconsumption.csv")
df["Datetime"] = pd.to_datetime(df["Datetime"])
df.rename(columns={"Datetime": "DateTime"}, inplace=True)
df = df.sort_values("DateTime")

df

# ===========================================
# 2. FEATURE ENGINEERING TEMPORAL
# ===========================================

# Fin de semana
df["is_weekend"] = df["DateTime"].dt.dayofweek.isin([5, 6]).astype(int)

# Festivos de Marruecos 2017
morocco_holidays_2017 = [
    "2017-01-01","2017-01-11",
    "2017-05-26","2017-05-27",
    "2017-06-25","2017-06-26",
    "2017-09-01","2017-09-02",
    "2017-07-30","2017-08-14","2017-08-20","2017-08-21",
    "2017-09-21","2017-11-06","2017-11-18"
]

holidays = pd.to_datetime(morocco_holidays_2017)
df["is_holiday"] = pd.to_datetime(df["DateTime"].dt.date).isin(holidays.date).astype(int)

# Otras variables temporales √∫tiles
df["hour"] = df["DateTime"].dt.hour
df["month"] = df["DateTime"].dt.month

# Agregaci√≥n horaria
df_hourly = df.resample("H", on="DateTime").mean().reset_index()

# Recalcular variables binarias al nivel horario
df_hourly["is_weekend"] = df_hourly["DateTime"].dt.dayofweek.isin([5, 6]).astype(int)
df_hourly["is_holiday"] = pd.to_datetime(df_hourly["DateTime"].dt.date).isin(holidays.date).astype(int)
df_hourly["hour"] = df_hourly["DateTime"].dt.hour
df_hourly["month"] = df_hourly["DateTime"].dt.month

df_hourly

# ===========================================
# 3. SELECCI√ìN DE FEATURES
# ===========================================
target_col = "PowerConsumption_Zone1"

# Variables num√©ricas principales (incluyendo meteorol√≥gicas)
num_features = [
    "PowerConsumption_Zone1",
    "Temperature",
    "Humidity",
    "WindSpeed",
    "GeneralDiffuseFlows",
    "DiffuseFlows",
    "hour",
    "month"
]

# Variables categ√≥ricas binarias
cat_features = ["is_weekend", "is_holiday"]

# Subconjunto del dataset con solo las columnas necesarias
df_model = df_hourly[num_features + cat_features].copy().dropna()

df_model

# ===========================================
# 4. SPLIT TRAIN/TEST (sin fuga de informaci√≥n)
# ===========================================
split_index = int(0.8 * len(df_model))
train_df = df_model.iloc[:split_index]
test_df  = df_model.iloc[split_index:]

# ===========================================
# 5. ESCALADO (solo num√©ricas)
# ===========================================
scaler = MinMaxScaler()
train_scaled_num = scaler.fit_transform(train_df[num_features])
test_scaled_num  = scaler.transform(test_df[num_features])

# Categ√≥ricas como enteros (no se escalan)
train_cat = train_df[cat_features].astype(int)
test_cat  = test_df[cat_features].astype(int)

# Combinar num√©ricas escaladas + categ√≥ricas
train_scaled = np.concatenate([train_scaled_num, train_cat.values], axis=1)
test_scaled  = np.concatenate([test_scaled_num, test_cat.values], axis=1)

train_scaled

test_scaled

# ===========================================
# 6. CREAR SECUENCIAS MULTIVARIADAS
# ===========================================
def create_sequences_multivariate(X, y, window):
    Xs, ys = [], []
    for i in range(len(X) - window):
        Xs.append(X[i:i+window])
        ys.append(y[i+window])
    return np.array(Xs), np.array(ys)

# Crear secuencias para predecir 1 hora adelante
window_sizes = [24, 48, 72]  # distintas longitudes de memoria

# ===========================================
# 7. FUNCI√ìN DE CONSTRUCCI√ìN DE MODELOS (con Dropout)
# ===========================================
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dense, Dropout
from tensorflow.keras.optimizers import Adam

def build_model(model_type, units1, units2, n_features, window, dropout_rate=0.2, lr=0.001):
    """
    Construye un modelo RNN, LSTM o GRU con Dropout opcional.
    Par√°metros:
        model_type: tipo de red ('RNN', 'LSTM' o 'GRU')
        units1, units2: n√∫mero de neuronas por capa
        n_features: n√∫mero de variables ex√≥genas (features)
        window: tama√±o de la ventana temporal
        dropout_rate: fracci√≥n de unidades a apagar (default=0.2)
        lr: tasa de aprendizaje
    """

    if model_type == "RNN":
        model = Sequential([
            SimpleRNN(units1, return_sequences=True, activation="tanh", input_shape=(window, n_features)),
            Dropout(dropout_rate),
            SimpleRNN(units2, activation="tanh"),
            Dense(1, activation="linear")
        ])

    elif model_type == "LSTM":
        model = Sequential([
            LSTM(units1, return_sequences=True, activation="tanh", input_shape=(window, n_features)),
            Dropout(dropout_rate),
            LSTM(units2, activation="tanh"),
            Dense(1, activation="linear")
        ])

    elif model_type == "GRU":
        model = Sequential([
            GRU(units1, return_sequences=True, activation="tanh", input_shape=(window, n_features)),
            Dropout(dropout_rate),
            GRU(units2, activation="tanh"),
            Dense(1, activation="linear")
        ])

    else:
        raise ValueError("Modelo no soportado. Usa 'RNN', 'LSTM' o 'GRU'.")

    model.compile(optimizer=Adam(learning_rate=lr), loss="mse")
    return model

# ===========================================
# 8. FUNCI√ìN DE EVALUACI√ìN
# ===========================================
def evaluate_model(model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)
    y_pred = model.predict(X_test, verbose=0)

    mae = mean_absolute_error(y_test, y_pred)
    rmse = sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-6))) * 100
    return mae, rmse, r2, mape

# ===========================================
# 9. EXPERIMENTOS
# ===========================================
architectures = [(50,50), (40,40), (40,20)]
models = ["RNN", "LSTM", "GRU"]
results = []

total = len(window_sizes) * len(models) * len(architectures)
count = 0

for window in window_sizes:
    # Crear secuencias sin fuga (solo con training data escalada)
    X_train, y_train = create_sequences_multivariate(train_scaled, train_scaled[:, 0], window)
    X_test, y_test   = create_sequences_multivariate(test_scaled, test_scaled[:, 0], window)

    n_features = X_train.shape[2]

    for model_type in models:
        for (u1, u2) in architectures:
            count += 1
            print(f"\nüöÄ Entrenando modelo {count}/{total}: {model_type} | ventana={window} | arquitectura=({u1},{u2})")

            model = build_model(model_type, u1, u2, n_features, window)

            # Entrenamiento
            history = model.fit(
                X_train, y_train,
                epochs=10,
                batch_size=32,
                validation_data=(X_test, y_test),
                verbose=0
            )

            # Predicciones
            y_pred = model.predict(X_test, verbose=0)

            # ==========================================================
            # ‚öôÔ∏è DES-ESCALAR para m√©tricas reales (sin fuga de informaci√≥n)
            # ==========================================================
            # reconstruimos con ceros en otras features
            y_test_inv = scaler.inverse_transform(
                np.concatenate([y_test.reshape(-1,1), np.zeros((len(y_test), len(num_features)-1))], axis=1)
            )[:,0]

            y_pred_inv = scaler.inverse_transform(
                np.concatenate([y_pred.reshape(-1,1), np.zeros((len(y_pred), len(num_features)-1))], axis=1)
            )[:,0]

            # ==========================================================
            # üìä M√âTRICAS EN ESCALA ORIGINAL
            # ==========================================================
            mae = mean_absolute_error(y_test_inv, y_pred_inv)
            rmse = sqrt(mean_squared_error(y_test_inv, y_pred_inv))
            r2 = r2_score(y_test_inv, y_pred_inv)
            mape = np.mean(np.abs((y_test_inv - y_pred_inv) / (y_test_inv + 1e-6))) * 100

            results.append([model_type, window, f"{u1}-{u2}", mae, rmse, r2, mape])

            print(f"‚úÖ Modelo {count}/{total} FINALIZADO ‚Üí {model_type} | ventana={window} | arquitectura=({u1},{u2})")
            print(f"    üîπ MAE={mae:.3f} | RMSE={rmse:.3f} | R¬≤={r2:.3f} | MAPE={mape:.2f}%")

            # ==========================================================
            # üìà CURVA DE P√âRDIDA (Loss vs √âpoca)
            # ==========================================================
            plt.figure(figsize=(8,4))
            plt.plot(history.history['loss'], label='Entrenamiento', linewidth=2)
            plt.plot(history.history['val_loss'], label='Validaci√≥n', linewidth=2)
            plt.title(f'Curva de p√©rdida - {model_type} (ventana={window}h, arquitectura=({u1},{u2}))')
            plt.xlabel('√âpoca')
            plt.ylabel('Error (MSE)')
            plt.legend()
            plt.grid(alpha=0.3)
            plt.tight_layout()
            plt.show()

print("\nüéØ TODOS LOS ENTRENAMIENTOS FINALIZADOS üéØ")

# Mostrar tabla de resultados
results_df = pd.DataFrame(results, columns=["Modelo", "Ventana", "Arquitectura", "MAE", "RMSE", "R2", "MAPE (%)"])
print("\n===== Resultados comparativos =====\n")
print(results_df.to_string(index=False))

# ===========================================
# 10. RESULTADOS
# ===========================================
results_df = pd.DataFrame(results, columns=["Modelo", "Ventana (h)", "Arquitectura", "MAE", "RMSE", "R2", "MAPE (%)"])
results_df = results_df.sort_values(by="MAPE (%)", ascending=True).reset_index(drop=True)

print("\n===== RESULTADOS COMPARATIVOS MULTIVARIADOS =====\n")
print(results_df.to_string(index=False))

best = results_df.iloc[0]
print("\n===== MEJOR CONFIGURACI√ìN =====")
print(f"Modelo: {best['Modelo']}, Ventana: {best['Ventana (h)']}h, Arquitectura: {best['Arquitectura']}")
print(f"MAE: {best['MAE']:.3f}, RMSE: {best['RMSE']:.3f}, R2: {best['R2']:.3f}, MAPE: {best['MAPE (%)']:.2f}%")


results_df.to_excel("resultados_grid_search.xlsx")

import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping

# ===========================================
# 11. GRAFICAR REAL vs PREDICHO (con EarlyStopping)
# ===========================================

# Entrenar nuevamente el mejor modelo (por si no lo guardaste)
best_window = 72
best_units = (40, 40)
best_model_type = "GRU"

# Crear secuencias
X_train, y_train = create_sequences_multivariate(train_scaled, train_scaled[:, 0], best_window)
X_test, y_test   = create_sequences_multivariate(test_scaled, test_scaled[:, 0], best_window)
n_features = X_train.shape[2]

# Crear el modelo
best_model = build_model(best_model_type, best_units[0], best_units[1], n_features, best_window)

# üìå CALLBACK: Early Stopping
early_stop = EarlyStopping(
    monitor='val_loss',     # m√©trica a monitorear
    patience=5,             # n√∫mero de √©pocas sin mejora antes de detener
    restore_best_weights=True,  # restaura los mejores pesos
    verbose=1
)

# Entrenamiento con validaci√≥n y EarlyStopping
history = best_model.fit(
    X_train, y_train,
    validation_split=0.2,   # 20% del entrenamiento para validaci√≥n
    epochs=50,
    batch_size=32,
    verbose=1,
    callbacks=[early_stop]
)

# Predicciones
y_pred = best_model.predict(X_test, verbose=0)

# Inversa del escalado (en la variable objetivo original)
y_test_inv = scaler.inverse_transform(
    np.concatenate([y_test.reshape(-1,1), np.zeros((len(y_test), len(num_features)-1))], axis=1)
)[:,0]

y_pred_inv = scaler.inverse_transform(
    np.concatenate([y_pred.reshape(-1,1), np.zeros((len(y_pred), len(num_features)-1))], axis=1)
)[:,0]

# Recalcular MAPE correctamente
mape_correct = np.mean(np.abs((y_test_inv - y_pred_inv) / (y_test_inv + 1e-6))) * 100
print(f"\n‚úÖ MAPE recalculado correctamente (en escala original): {mape_correct:.2f}%")

# Graficar comparaci√≥n real vs predicho
plt.figure(figsize=(12,5))
plt.plot(y_test_inv, label='Real', color='black', linewidth=1.5)
plt.plot(y_pred_inv, label='Predicho', color='orange', linestyle='--', linewidth=1.5)
plt.title(f'Comparaci√≥n Real vs Predicho - {best_model_type} (ventana={best_window}h)')
plt.xlabel('Tiempo (horas)')
plt.ylabel('Consumo energ√©tico')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# üìà Graficar curva de p√©rdida
plt.figure(figsize=(8,4))
plt.plot(history.history['loss'], label='Entrenamiento', linewidth=2)
plt.plot(history.history['val_loss'], label='Validaci√≥n', linewidth=2)
plt.xlabel('√âpocas')
plt.ylabel('P√©rdida')
plt.legend()
plt.title('Curva de p√©rdida con Early Stopping')
plt.grid(True, alpha=0.3)
plt.show()

"""# Algoritmo gen√©tico"""

!pip install deap

# ===========================================
# üß¨ OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS CON GA (versi√≥n inspirada en Oh et al., 2022)
# ===========================================

import random
import numpy as np
import tensorflow as tf
import warnings
from deap import base, creator, tools
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from math import sqrt
import matplotlib.pyplot as plt
import pandas as pd

# Suprimir warnings de Keras
warnings.filterwarnings("ignore", category=UserWarning)


# ===========================================
# 1Ô∏è‚É£ FUNCI√ìN DE EVALUACI√ìN (con control de l√≠mites)
# ===========================================

def evaluate(individual):
    """Eval√∫a un conjunto de hiperpar√°metros y devuelve el fitness = 1/(1 + MAPE)."""
    model_type, units1, units2, window, batch_size, lr = individual

    # üîí Control de l√≠mites v√°lidos
    units1 = int(np.clip(units1, 10, 120))
    units2 = int(np.clip(units2, 10, 120))
    window = int(np.clip(window, 12, 120))
    batch_size = int(np.clip(batch_size, 8, 256))
    lr = float(np.clip(lr, 1e-5, 1e-2))

    try:
        # Crear secuencias
        X_train, y_train = create_sequences_multivariate(train_scaled, train_scaled[:, 0], window)
        X_test, y_test   = create_sequences_multivariate(test_scaled, test_scaled[:, 0], window)
        n_features = X_train.shape[2]

        # Crear modelo
        model = build_model(model_type, units1, units2, n_features, window)
        model.compile(optimizer=Adam(learning_rate=lr), loss="mse")

        # Entrenamiento r√°pido
        model.fit(X_train, y_train, epochs=10, batch_size=batch_size, verbose=0)

        # Predicciones
        y_pred = model.predict(X_test, verbose=0)

        # Inversa del escalado (solo variable objetivo)
        y_test_inv = scaler.inverse_transform(
            np.concatenate([y_test.reshape(-1,1), np.zeros((len(y_test), len(num_features)-1))], axis=1)
        )[:,0]
        y_pred_inv = scaler.inverse_transform(
            np.concatenate([y_pred.reshape(-1,1), np.zeros((len(y_pred), len(num_features)-1))], axis=1)
        )[:,0]

        # Calcular MAPE
        mape = np.mean(np.abs((y_test_inv - y_pred_inv) / (y_test_inv + 1e-6))) * 100

        if np.isnan(mape) or np.isinf(mape):
            mape = 9999  # penalizaci√≥n si algo sale mal

    except Exception as e:
        print(f"‚ö†Ô∏è Error en evaluaci√≥n: {e}")
        mape = 9999  # penalizaci√≥n por fallo

    # Liberar memoria GPU
    tf.keras.backend.clear_session()

    return (1 / (1 + mape),)


# ===========================================
# 2Ô∏è‚É£ CONFIGURACI√ìN DEL GA
# ===========================================

# Reiniciar creators si ya existen
if "creator" in globals():
    try:
        del creator.FitnessMax
        del creator.Individual
    except:
        pass

creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

toolbox = base.Toolbox()

# Espacio de b√∫squeda
model_types = ["RNN", "LSTM", "GRU"]
toolbox.register("attr_model", random.choice, model_types)
toolbox.register("attr_units1", random.randint, 20, 80)
toolbox.register("attr_units2", random.randint, 20, 80)
toolbox.register("attr_window", random.randint, 24, 72)
toolbox.register("attr_batch", random.randint, 32, 128)
toolbox.register("attr_lr", random.uniform, 1e-4, 5e-3)

# Crear individuo y poblaci√≥n
toolbox.register("individual", tools.initCycle, creator.Individual,
                 (toolbox.attr_model, toolbox.attr_units1, toolbox.attr_units2,
                  toolbox.attr_window, toolbox.attr_batch, toolbox.attr_lr), n=1)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)


# ===========================================
# 3Ô∏è‚É£ OPERADORES GEN√âTICOS (seg√∫n el paper)
# ===========================================

def crossover_exponent(ind1, ind2, alpha=0.4, exponent=2):
    """Crossover con exponente (como en Oh et al., 2022)."""
    if random.random() < 0.5:
        ind1[0], ind2[0] = ind2[0], ind1[0]
    for i in range(1, len(ind1)):
        gamma = ((1 + 2 * alpha) * random.random() - alpha) ** exponent
        ind1[i] = (1 - gamma) * ind1[i] + gamma * ind2[i]
        ind2[i] = gamma * ind1[i] + (1 - gamma) * ind2[i]

        # üîí Evitar valores fuera de rango
        if i in [1, 2]:  # units
            ind1[i] = np.clip(ind1[i], 10, 120)
            ind2[i] = np.clip(ind2[i], 10, 120)
        elif i == 3:  # window
            ind1[i] = np.clip(ind1[i], 12, 120)
            ind2[i] = np.clip(ind2[i], 12, 120)
        elif i == 4:  # batch
            ind1[i] = np.clip(ind1[i], 8, 256)
            ind2[i] = np.clip(ind2[i], 8, 256)
        elif i == 5:  # lr
            ind1[i] = np.clip(ind1[i], 1e-5, 1e-2)
            ind2[i] = np.clip(ind2[i], 1e-5, 1e-2)
    return ind1, ind2


def adaptive_mutation(individual):
    """Mutaci√≥n gaussiana adaptativa (paper Oh et al., 2022)."""
    sigma_ratios = [0.1, 0.1, 0.1, 0.05, 0.3]  # escala relativa
    param_ranges = [
        (20, 80),    # units1
        (20, 80),    # units2
        (24, 72),    # window
        (32, 128),   # batch
        (1e-4, 5e-3) # lr
    ]

    if random.random() < 0.1:
        individual[0] = random.choice(model_types)

    for i in range(1, len(individual)):
        if random.random() < 0.3:
            low, high = param_ranges[i-1]
            sigma = (high - low) * sigma_ratios[i-1]
            individual[i] += random.gauss(0, sigma)
            individual[i] = np.clip(individual[i], low, high)

    return (individual,)


# Registrar operadores
toolbox.register("mate", crossover_exponent)
toolbox.register("mutate", adaptive_mutation)
toolbox.register("select", tools.selTournament, tournsize=3)
toolbox.register("evaluate", evaluate)


# ===========================================
# 4Ô∏è‚É£ EJECUCI√ìN DEL GA
# ===========================================

pop = toolbox.population(n=10)
NGEN = 30

best_fitness, mean_fitness, worst_fitness = [], [], []
log_data = []

print("\nüöÄ Iniciando optimizaci√≥n gen√©tica (versi√≥n robusta Oh et al., 2022)...\n")

for gen in range(NGEN):
    offspring = toolbox.select(pop, len(pop))
    offspring = list(map(toolbox.clone, offspring))

    # Cruce
    for child1, child2 in zip(offspring[::2], offspring[1::2]):
        if random.random() < 0.7:
            toolbox.mate(child1, child2)
            del child1.fitness.values
            del child2.fitness.values

    # Mutaci√≥n
    for mutant in offspring:
        if random.random() < 0.4:
            toolbox.mutate(mutant)
            del mutant.fitness.values

    # Evaluaci√≥n
    invalid = [ind for ind in offspring if not ind.fitness.valid]
    fitnesses = list(map(toolbox.evaluate, invalid))
    for ind, fit in zip(invalid, fitnesses):
        ind.fitness.values = fit

    pop[:] = offspring

    fits = [ind.fitness.values[0] for ind in pop]
    best_fitness.append(np.max(fits))
    mean_fitness.append(np.mean(fits))
    worst_fitness.append(np.min(fits))

    best_ind = tools.selBest(pop, 1)[0]
    print(f"üîπ Generaci√≥n {gen+1}/{NGEN} | Mejor fitness: {np.max(fits):.4f} | Modelo: {best_ind[0]}")

    # Guardar resultados
    log_data.append({
        "Generaci√≥n": gen+1,
        "Modelo": best_ind[0],
        "units1": int(best_ind[1]),
        "units2": int(best_ind[2]),
        "window": int(best_ind[3]),
        "batch": int(best_ind[4]),
        "lr": best_ind[5],
        "Fitness": np.max(fits)
    })


# ===========================================
# 5Ô∏è‚É£ RESULTADOS FINALES
# ===========================================

best_ind = tools.selBest(pop, 1)[0]
print(f"\nüèÜ Mejor individuo encontrado:")
print(f"   ‚Üí Modelo: {best_ind[0]}")
print(f"   ‚Üí units1={int(best_ind[1])}, units2={int(best_ind[2])}")
print(f"   ‚Üí window={int(best_ind[3])}h, batch={int(best_ind[4])}, lr={best_ind[5]:.5f}")

results_df = pd.DataFrame(log_data)
display(results_df)

# ===========================================
# 6Ô∏è‚É£ GR√ÅFICO DE EVOLUCI√ìN
# ===========================================

plt.figure(figsize=(8,4))
plt.plot(best_fitness, label='Mejor Fitness', color='green', linewidth=2)
plt.plot(mean_fitness, label='Fitness Promedio', color='blue', linestyle='--')
plt.plot(worst_fitness, label='Peor Fitness', color='red', linestyle=':')
plt.title("Evoluci√≥n del Fitness - GA (basado en Oh et al., 2022)")
plt.xlabel("Generaci√≥n")
plt.ylabel("Fitness (1 / (1 + MAPE))")
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# ===========================================
# üîÅ REENTRENAR EL MEJOR MODELO (OPTIMIZADO Y LIMPIO)
# ===========================================

import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from math import sqrt
import matplotlib.pyplot as plt
import numpy as np

# ===========================================
# 1Ô∏è‚É£ LIMPIAR SESI√ìN DE TENSORFLOW (previene errores de SHAP)
# ===========================================
import gc
gc.collect()
tf.keras.backend.clear_session()

# ===========================================
# 2Ô∏è‚É£ CONFIGURACI√ìN DE LOS MEJORES HIPERPAR√ÅMETROS
# ===========================================
best_model_type = "GRU"
best_units1 = 39
best_units2 = 31
best_window = 57
best_batch = 107
best_lr = 0.004814

# ===========================================
# 3Ô∏è‚É£ RECONSTRUIR SECUENCIAS
# ===========================================
X_train, y_train = create_sequences_multivariate(train_scaled, train_scaled[:, 0], best_window)
X_test, y_test   = create_sequences_multivariate(test_scaled, test_scaled[:, 0], best_window)
n_features = X_train.shape[2]

# ===========================================
# 4Ô∏è‚É£ CONSTRUIR Y COMPILAR EL MODELO
# ===========================================
best_model = build_model(best_model_type, best_units1, best_units2, n_features, best_window)
best_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=best_lr),
    loss='mse'
)

# ===========================================
# 5Ô∏è‚É£ ENTRENAMIENTO CON EARLY STOPPING
# ===========================================
es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = best_model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=50,
    batch_size=best_batch,
    callbacks=[es],
    verbose=1
)

# ===========================================
# 6Ô∏è‚É£ PREDICCIONES Y M√âTRICAS EN ESCALA ORIGINAL
# ===========================================
y_pred = best_model.predict(X_test, verbose=0)

# Inversa del escalado (solo variable objetivo)
y_test_inv = scaler.inverse_transform(
    np.concatenate([y_test.reshape(-1,1), np.zeros((len(y_test), len(num_features)-1))], axis=1)
)[:,0]

y_pred_inv = scaler.inverse_transform(
    np.concatenate([y_pred.reshape(-1,1), np.zeros((len(y_pred), len(num_features)-1))], axis=1)
)[:,0]

# M√©tricas
mae = mean_absolute_error(y_test_inv, y_pred_inv)
rmse = sqrt(mean_squared_error(y_test_inv, y_pred_inv))
r2 = r2_score(y_test_inv, y_pred_inv)
mape = np.mean(np.abs((y_test_inv - y_pred_inv) / (y_test_inv + 1e-6))) * 100

print("\n===== üìä M√âTRICAS FINALES EN TEST =====")
print(f"MAE :  {mae:.3f}")
print(f"RMSE:  {rmse:.3f}")
print(f"R¬≤  :  {r2:.3f}")
print(f"MAPE:  {mape:.2f}%")

# ===========================================
# 7Ô∏è‚É£ GR√ÅFICOS DE ENTRENAMIENTO Y PREDICCI√ìN
# ===========================================

# Curva de p√©rdida
plt.figure(figsize=(8,4))
plt.plot(history.history['loss'], label='Entrenamiento', linewidth=2)
plt.plot(history.history['val_loss'], label='Validaci√≥n', linewidth=2, linestyle='--')
plt.title('Curva de p√©rdida (Loss vs √âpocas)')
plt.xlabel('√âpocas')
plt.ylabel('MSE')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# Real vs Predicho
plt.figure(figsize=(10,5))
plt.plot(y_test_inv, label='Real', color='black', linewidth=1.5)
plt.plot(y_pred_inv, label='Predicho', color='orange', linestyle='--', linewidth=1.5)
plt.title(f'Comparaci√≥n Real vs Predicho - {best_model_type} (ventana={best_window})')
plt.xlabel('Tiempo (√≠ndice de muestra)')
plt.ylabel('Consumo energ√©tico')
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print("\n‚úÖ Entrenamiento completado correctamente.")

